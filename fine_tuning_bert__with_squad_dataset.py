# -*- coding: utf-8 -*-
"""Fine_tuning_bert_ with_squad_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ww0lodzf5opvo_WQ6Lm3Pc8FNjX98H6C
"""

# import os
# os.kill(os.getpid(), 9)

import wandb
from huggingface_hub import login

# Hugging Face and W&B API Keys
hugging_face_key = "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"
wandb_api_key = "9974aaae099d5a49bd33fdc8498d4be48f56531a"

# Logging into Hugging Face using the API key
login(token=hugging_face_key)

# Set up W&B for logging
wandb.login(key=wandb_api_key)



# Install necessary packages
!pip install transformers datasets peft accelerate bitsandbytes

import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset
from peft import LoraConfig, get_peft_model

# Step 1: Load the SQuAD dataset
dataset = load_dataset("squad")

# Step 2: Load the tokenizer and the BERT model (use smaller model for faster training)
model_id = "bert-base-uncased"  # Use smaller model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForQuestionAnswering.from_pretrained(model_id)

# Step 3: Preprocess the dataset
def preprocess_function(examples):
    tokenized_examples = tokenizer(
        examples["question"], examples["context"], truncation=True, padding="max_length", max_length=512
    )

    # Find the start and end positions of the answers in the context
    start_positions = []
    end_positions = []

    for i in range(len(examples["answers"])):
        context = examples["context"][i]
        answer = examples["answers"][i]["text"][0]  # Assuming there's only one answer per question
        start_pos = context.find(answer)
        end_pos = start_pos + len(answer) - 1

        start_positions.append(start_pos)
        end_positions.append(end_pos)

    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples

# Apply preprocessing
train_dataset = dataset["train"].map(preprocess_function, batched=True)
validation_dataset = dataset["validation"].map(preprocess_function, batched=True)

# Step 4: Apply LoRA (PEFT) to the model
lora_config = LoraConfig(
    r=8,  # Low-rank decomposition rank
    lora_alpha=32,  # Scaling factor for LoRA
    lora_dropout=0.1,  # Dropout rate for LoRA
    task_type="QUESTION_ANS",  # LoRA task type for Question Answering
)

# Get the LoRA model
model = get_peft_model(model, lora_config)

# Step 5: Define the training arguments
training_args = TrainingArguments(
    output_dir="./results",  # Output directory for the model checkpoints
    evaluation_strategy="epoch",  # Evaluate after each epoch
    learning_rate=2e-5,  # Learning rate
    per_device_train_batch_size=4,  # Batch size for training
    per_device_eval_batch_size=4,  # Batch size for evaluation
    num_train_epochs=2,  # Number of epochs (adjust as needed)
    weight_decay=0.01,  # Weight decay for regularization
    save_steps=10_000,  # Save checkpoint every 10k steps
    logging_dir="./logs",  # Directory for storing logs
    logging_steps=500,  # Log every 500 steps
    fp16=True,  # Use mixed precision training
    gradient_accumulation_steps=2,  # Simulate larger batch size
)

# Step 6: Create the Trainer object
trainer = Trainer(
    model=model,  # Model for fine-tuning
    args=training_args,  # Training arguments
    train_dataset=train_dataset,  # Training dataset
    eval_dataset=validation_dataset,  # Validation dataset
    tokenizer=tokenizer,  # Tokenizer for preprocessing
)

# Step 7: Start training
trainer.train()

# Step 8: Save the fine-tuned model
model.save_pretrained("./fine_tuned_lora_bert_squad")
tokenizer.save_pretrained("./fine_tuned_lora_bert_squad")

# Step 9: Evaluate the model
eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}")

# Step 10: Use the fine-tuned model for Question Answering
def answer_question(question, context):
    inputs = tokenizer(question, context, return_tensors="pt")
    answer_start_scores, answer_end_scores = model(**inputs)

    # Find the start and end of the answer span
    answer_start = answer_start_scores.argmax()
    answer_end = answer_end_scores.argmax()

    # Decode the answer
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end + 1]))
    return answer

# Example usage: Answering a question using the fine-tuned LoRA model
context = "Hugging Face is creating a tool that democratizes AI."
question = "What is Hugging Face creating?"
answer = answer_question(question, context)
print(f"Answer: {answer}")